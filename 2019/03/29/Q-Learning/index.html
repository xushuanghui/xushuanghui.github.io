<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Q-Learning-1 | xushuanghui</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="###Q-Learning 整体算法   Q learning 的算法, 每次更新我们都用到了 Q 现实和 Q 估计, 而且 Q learning 的迷人之处就是 在 Q(s1, a2) 现实 中, 也包含了一个 Q(s2) 的最大估计值, 将对下一步的衰减的最大估计和当前所得到的奖励当成这一步的现实, 很奇妙吧. 最后我们来说说这套算法中一些参数的意义. Epsilon greedy 是用在决策">
<meta property="og:type" content="article">
<meta property="og:title" content="Q-Learning-1">
<meta property="og:url" content="http://example.com/2019/03/29/Q-Learning/index.html">
<meta property="og:site_name" content="xushuanghui">
<meta property="og:description" content="###Q-Learning 整体算法   Q learning 的算法, 每次更新我们都用到了 Q 现实和 Q 估计, 而且 Q learning 的迷人之处就是 在 Q(s1, a2) 现实 中, 也包含了一个 Q(s2) 的最大估计值, 将对下一步的衰减的最大估计和当前所得到的奖励当成这一步的现实, 很奇妙吧. 最后我们来说说这套算法中一些参数的意义. Epsilon greedy 是用在决策">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://morvanzhou.github.io/static/results/ML-intro/q4.png">
<meta property="og:image" content="https://morvanzhou.github.io/static/results/ML-intro/q3.png">
<meta property="og:image" content="https://morvanzhou.github.io/static/results/ML-intro/q5.png">
<meta property="og:image" content="https://morvanzhou.github.io/static/results/reinforcement-learning/2-1-1.png">
<meta property="article:published_time" content="2019-03-29T12:06:02.000Z">
<meta property="article:modified_time" content="2020-12-16T03:15:42.402Z">
<meta property="article:author" content="xushuanghui">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://morvanzhou.github.io/static/results/ML-intro/q4.png">
  
    <link rel="alternate" href="/atom.xml" title="xushuanghui" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">xushuanghui</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Q-Learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/03/29/Q-Learning/" class="article-date">
  <time class="dt-published" datetime="2019-03-29T12:06:02.000Z" itemprop="datePublished">2019-03-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Q-Learning-1
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>###Q-Learning 整体算法</p>
<p><a target="_blank" rel="noopener" href="https://morvanzhou.github.io/static/results/ML-intro/q4.png"><img src="https://morvanzhou.github.io/static/results/ML-intro/q4.png" alt="Q Leaning"></a></p>
<p> Q learning 的算法, 每次更新我们都用到了 Q 现实和 Q 估计, 而且 Q learning 的迷人之处就是 在 Q(s1, a2) 现实 中, 也包含了一个 Q(s2) 的最大估计值, 将对下一步的衰减的最大估计和当前所得到的奖励当成这一步的现实, 很奇妙吧. 最后我们来说说这套算法中一些参数的意义. Epsilon greedy 是用在决策上的一种策略, 比如 epsilon &#x3D; 0.9 时, 就说明有90% 的情况我会按照 Q 表的最优值选择行为, 10% 的时间使用随机选行为. alpha是学习率, 来决定这次的误差有多少是要被学习的, alpha是一个小于1 的数. gamma 是对未来 reward 的衰减值. 我们可以这样想象.</p>
<span id="more"></span>



<h2 id="Q-Learning-更新"><a href="#Q-Learning-更新" class="headerlink" title="Q-Learning 更新"></a>Q-Learning 更新</h2><p><a target="_blank" rel="noopener" href="https://morvanzhou.github.io/static/results/ML-intro/q3.png"><img src="https://morvanzhou.github.io/static/results/ML-intro/q3.png" alt="Q Leaning"></a></p>
<h2 id="Q-Learning-Gamma"><a href="#Q-Learning-Gamma" class="headerlink" title="Q-Learning Gamma"></a>Q-Learning Gamma</h2><p><img src="https://morvanzhou.github.io/static/results/ML-intro/q5.png" alt="gamma"></p>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子:"></a>例子:</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-o---T</span><br><span class="line"># T 就是宝藏的位置, o 是探索者的位置</span><br></pre></td></tr></table></figure>

<p>Q-learning 是一种记录行为值 (Q value) 的方法, 每种在一定状态的行为都会有一个值 <code>Q(s, a)</code>, 就是说 行为 <code>a</code> 在 <code>s</code> 状态的值是 <code>Q(s, a)</code>. <code>s</code> 在上面的探索者游戏中, 就是 <code>o</code> 所在的地点了. 而每一个地点探索者都能做出两个行为 <code>left/right</code>, 这就是探索者的所有可行的 <code>a</code> 啦.</p>
<p>如果在某个地点 <code>s1</code>, 探索者计算了他能有的两个行为, <code>a1/a2=left/right</code>, 计算结果是 <code>Q(s1, a1) &gt; Q(s1, a2)</code>, 那么探索者就会选择 <code>left</code> 这个行为. 这就是 Q learning 的行为选择简单规则.</p>
<h2 id="预设值"><a href="#预设值" class="headerlink" title="预设值"></a>预设值</h2><p>这一次需要的模块和参数设置:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">N_STATES = 6   # 1维世界的宽度</span><br><span class="line">ACTIONS = [&#x27;left&#x27;, &#x27;right&#x27;]     # 探索者的可用动作</span><br><span class="line">EPSILON = 0.9   # 贪婪度 greedy</span><br><span class="line">ALPHA = 0.1     # 学习率</span><br><span class="line">GAMMA = 0.9    # 奖励递减值</span><br><span class="line">MAX_EPISODES = 13   # 最大回合数</span><br><span class="line">FRESH_TIME = 0.3    # 移动间隔时间</span><br></pre></td></tr></table></figure>

<h2 id="Q-表"><a href="#Q-表" class="headerlink" title="Q 表"></a>Q 表</h2><p>对于 tabular Q learning, 我们必须将所有的 Q values (行为值) 放在 <code>q_table</code> 中, 更新 <code>q_table</code> 也是在更新他的行为准则. <code>q_table</code> 的 index 是所有对应的 <code>state</code> (探索者位置), columns 是对应的 <code>action</code> (探索者行为).</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def build_q_table(n_states, actions):</span><br><span class="line">    table = pd.DataFrame(</span><br><span class="line">        np.zeros((n_states, len(actions))),     # q_table 全 0 初始</span><br><span class="line">        columns=actions,    # columns 对应的是行为名称</span><br><span class="line">    )</span><br><span class="line">    return table</span><br><span class="line"></span><br><span class="line"># q_table:</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">   left  right</span><br><span class="line">0   0.0    0.0</span><br><span class="line">1   0.0    0.0</span><br><span class="line">2   0.0    0.0</span><br><span class="line">3   0.0    0.0</span><br><span class="line">4   0.0    0.0</span><br><span class="line">5   0.0    0.0</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>

<h2 id="定义动作"><a href="#定义动作" class="headerlink" title="定义动作"></a>定义动作</h2><p>接着定义探索者是如何挑选行为的. 这是我们引入 <code>epsilon greedy</code> 的概念. 因为在初始阶段, 随机的探索环境, 往往比固定的行为模式要好, 所以这也是累积经验的阶段, 我们希望探索者不会那么贪婪(greedy). 所以 <code>EPSILON</code> 就是用来控制贪婪程度的值. <code>EPSILON</code> 可以随着探索时间不断提升(越来越贪婪), 不过在这个例子中, 我们就固定成 <code>EPSILON = 0.9</code>, 90% 的时间是选择最优策略, 10% 的时间来探索.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 在某个 state 地点, 选择行为</span><br><span class="line">def choose_action(state, q_table):</span><br><span class="line">    state_actions = q_table.iloc[state, :]  # 选出这个 state 的所有 action 值</span><br><span class="line">    if (np.random.uniform() &gt; EPSILON) or (state_actions.all() == 0):  # 非贪婪 or 或者这个 state 还没有探索过</span><br><span class="line">        action_name = np.random.choice(ACTIONS)</span><br><span class="line">    else:</span><br><span class="line">        action_name = state_actions.argmax()    # 贪婪模式</span><br><span class="line">    return action_name</span><br></pre></td></tr></table></figure>

<h2 id="环境反馈-S-R"><a href="#环境反馈-S-R" class="headerlink" title="环境反馈 S_, R"></a>环境反馈 S_, R</h2><p>做出行为后, 环境也要给我们的行为一个反馈, 反馈出下个 state (S_) 和 在上个 state (S) 做出 action (A) 所得到的 reward (R). 这里定义的规则就是, 只有当 <code>o</code> 移动到了 <code>T</code>, 探索者才会得到唯一的一个奖励, 奖励值 R&#x3D;1, 其他情况都没有奖励.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def get_env_feedback(S, A):</span><br><span class="line">    # This is how agent will interact with the environment</span><br><span class="line">    if A == &#x27;right&#x27;:    # move right</span><br><span class="line">        if S == N_STATES - 2:   # terminate</span><br><span class="line">            S_ = &#x27;terminal&#x27;</span><br><span class="line">            R = 1</span><br><span class="line">        else:</span><br><span class="line">            S_ = S + 1</span><br><span class="line">            R = 0</span><br><span class="line">    else:   # move left</span><br><span class="line">        R = 0</span><br><span class="line">        if S == 0:</span><br><span class="line">            S_ = S  # reach the wall</span><br><span class="line">        else:</span><br><span class="line">            S_ = S - 1</span><br><span class="line">    return S_, R</span><br></pre></td></tr></table></figure>

<h2 id="环境更新"><a href="#环境更新" class="headerlink" title="环境更新"></a>环境更新</h2><p>接下来就是环境的更新了, 不用细看.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def update_env(S, episode, step_counter):</span><br><span class="line">    # This is how environment be updated</span><br><span class="line">    env_list = [&#x27;-&#x27;]*(N_STATES-1) + [&#x27;T&#x27;]   # &#x27;---------T&#x27; our environment</span><br><span class="line">    if S == &#x27;terminal&#x27;:</span><br><span class="line">        interaction = &#x27;Episode %s: total_steps = %s&#x27; % (episode+1, step_counter)</span><br><span class="line">        print(&#x27;\r&#123;&#125;&#x27;.format(interaction), end=&#x27;&#x27;)</span><br><span class="line">        time.sleep(2)</span><br><span class="line">        print(&#x27;\r                                &#x27;, end=&#x27;&#x27;)</span><br><span class="line">    else:</span><br><span class="line">        env_list[S] = &#x27;o&#x27;</span><br><span class="line">        interaction = &#x27;&#x27;.join(env_list)</span><br><span class="line">        print(&#x27;\r&#123;&#125;&#x27;.format(interaction), end=&#x27;&#x27;)</span><br><span class="line">        time.sleep(FRESH_TIME)</span><br></pre></td></tr></table></figure>

<h2 id="强化学习主循环"><a href="#强化学习主循环" class="headerlink" title="强化学习主循环"></a>强化学习主循环</h2><p>最重要的地方就在这里. 你定义的 RL 方法都在这里体现. 在之后的教程中, 我们会更加详细得讲解 RL 中的各种方法, 下面的内容, 大家大概看看就行, 这节内容不用仔细研究.</p>
<p><a target="_blank" rel="noopener" href="https://morvanzhou.github.io/static/results/reinforcement-learning/2-1-1.png"><img src="https://morvanzhou.github.io/static/results/reinforcement-learning/2-1-1.png" alt="小例子"></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def rl():</span><br><span class="line">    q_table = build_q_table(N_STATES, ACTIONS)  # 初始 q table</span><br><span class="line">    for episode in range(MAX_EPISODES):     # 回合</span><br><span class="line">        step_counter = 0</span><br><span class="line">        S = 0   # 回合初始位置</span><br><span class="line">        is_terminated = False   # 是否回合结束</span><br><span class="line">        update_env(S, episode, step_counter)    # 环境更新</span><br><span class="line">        while not is_terminated:</span><br><span class="line"></span><br><span class="line">            A = choose_action(S, q_table)   # 选行为</span><br><span class="line">            S_, R = get_env_feedback(S, A)  # 实施行为并得到环境的反馈</span><br><span class="line">            q_predict = q_table.loc[S, A]    # 估算的(状态-行为)值</span><br><span class="line">            if S_ != &#x27;terminal&#x27;:</span><br><span class="line">                q_target = R + GAMMA * q_table.iloc[S_, :].max()   #  实际的(状态-行为)值 (回合没结束)</span><br><span class="line">            else:</span><br><span class="line">                q_target = R     #  实际的(状态-行为)值 (回合结束)</span><br><span class="line">                is_terminated = True    # terminate this episode</span><br><span class="line"></span><br><span class="line">            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  #  q_table 更新</span><br><span class="line">            S = S_  # 探索者移动到下一个 state</span><br><span class="line"></span><br><span class="line">            update_env(S, episode, step_counter+1)  # 环境更新</span><br><span class="line"></span><br><span class="line">            step_counter += 1</span><br><span class="line">    return q_table</span><br></pre></td></tr></table></figure>

<p>写好所有的评估和更新准则后, 我们就能开始训练了, 把探索者丢到环境中, 让它自己去玩吧.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    q_table = rl()</span><br><span class="line">    print(&#x27;\r\nQ-table:\n&#x27;)</span><br><span class="line">    print(q_table)</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/03/29/Q-Learning/" data-id="clgovw0nb000gk9d3d6vi45em" data-title="Q-Learning-1" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/04/01/Q-Learning-2/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Q-Learning-2
        
      </div>
    </a>
  
  
    <a href="/2019/03/26/GitLab-CI/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">GitLab CI</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Xcode/" rel="tag">Xcode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cocoaPods/" rel="tag">cocoaPods</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flutter/" rel="tag">flutter</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/iOS/" rel="tag">iOS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/iOS-AppDelegate/" rel="tag">iOS AppDelegate</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%87%8F%E5%8C%96%E4%BA%A4%E6%98%93/" rel="tag">量化交易</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Xcode/" style="font-size: 10px;">Xcode</a> <a href="/tags/cocoaPods/" style="font-size: 10px;">cocoaPods</a> <a href="/tags/flutter/" style="font-size: 10px;">flutter</a> <a href="/tags/iOS/" style="font-size: 20px;">iOS</a> <a href="/tags/iOS-AppDelegate/" style="font-size: 10px;">iOS AppDelegate</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">强化学习</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">机器学习</a> <a href="/tags/%E9%87%8F%E5%8C%96%E4%BA%A4%E6%98%93/" style="font-size: 10px;">量化交易</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">January 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">August 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/07/">July 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">August 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/04/20/init/">init</a>
          </li>
        
          <li>
            <a href="/2023/01/11/%E5%AE%B9%E5%99%A8%E5%8C%96/">首页容器化</a>
          </li>
        
          <li>
            <a href="/2022/08/14/%E5%8A%A8%E6%80%81%E5%BA%93%E6%87%92%E5%8A%A0%E8%BD%BD/">动态库懒加载</a>
          </li>
        
          <li>
            <a href="/2022/03/17/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E9%97%AE%E9%A2%98%E4%B8%8E%E6%B2%BB%E7%90%86/">多线程问题专项治理</a>
          </li>
        
          <li>
            <a href="/2021/07/20/%E7%9B%B4%E6%92%AD%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87%E7%9B%91%E6%8E%A7%E6%96%B9%E6%A1%88/">直播性能指标监控方案</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 xushuanghui<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>